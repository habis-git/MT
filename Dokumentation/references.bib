% Encoding: UTF-8

@Book{XAI2018,
  title     = {Explainable and Interpretable Models in Computer Vision and Machine Learning},
  publisher = {Springer-Verlag GmbH},
  year      = {2018},
  isbn      = {3319981307},
  date      = {2018-09-01},
  ean       = {9783319981307},
  keywords  = {rank3},
  url       = {https://www.ebook.de/de/product/33610206/explainable_and_interpretable_models_in_computer_vision_and_machine_learning.html},
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@article{Everingham_thepascal,
    author = {M. Everingham and L. Van Gool and C. K. I. Williams and J. Winn and A. Zisserman},
    title = { The PASCAL Visual Object Classes (VOC) Challenge},
    year = {2010}
}

@Article{Ras2018,
  author    = {GabriÃ«lle Ras and Marcel van Gerven and Pim Haselager},
  title     = {Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges},
  year      = {2018},
  pages     = {19--36},
  booktitle = {The Springer Series on Challenges in Machine Learning},
  doi       = {10.1007/978-3-319-98131-4_2},
  publisher = {Springer International Publishing},
}

@Article{Oh2019,
  author    = {Seong Joon Oh and Bernt Schiele and Mario Fritz},
  title     = {Towards Reverse-Engineering Black-Box Neural Networks},
  year      = {2019},
  pages     = {121--144},
  booktitle = {Explainable {AI}: Interpreting, Explaining and Visualizing Deep Learning},
  doi       = {10.1007/978-3-030-28954-6_7},
  publisher = {Springer International Publishing},
}

@Article{Selvaraju2016,
  author      = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  title       = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  abstract    = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  date        = {2016-10-07},
  doi         = {10.1007/s11263-019-01228-7},
  eprint      = {http://arxiv.org/abs/1610.02391v4},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1610.02391v4:PDF},
  keywords    = {cs.CV, cs.AI, cs.LG},
}

@Article{Simonyan2014,
  author      = {Karen Simonyan and Andrew Zisserman},
  title       = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  abstract    = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  date        = {2014-09-04},
  eprint      = {http://arxiv.org/abs/1409.1556v6},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1409.1556v6:PDF},
  keywords    = {cs.CV},
}

@Article{Ribeiro2016,
  author    = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  title     = {"Why Should I Trust You?"},
  year      = {2016},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} {\textquotesingle}16},
  doi       = {10.1145/2939672.2939778},
  publisher = {{ACM} Press},
}

@Article{Kim2017,
  author       = {Been Kim and Martin Wattenberg and Justin Gilmer and Carrie Cai and James Wexler and Fernanda Viegas and Rory Sayres},
  title        = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
  abstract     = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  date         = {2017-11-30},
  eprint       = {http://arxiv.org/abs/1711.11279v5},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1711.11279v5:PDF},
  journaltitle = {ICML 2018},
  keywords     = {stat.ML},
}

@Article{Friedman2008,
  author       = {Jerome H. Friedman and Bogdan E. Popescu},
  title        = {Predictive learning via rule ensembles},
  abstract     = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
  date         = {2008-11-11},
  doi          = {10.1214/07-AOAS148},
  eprint       = {http://arxiv.org/abs/0811.1679v1},
  eprintclass  = {stat.AP},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/0811.1679v1:PDF},
  journaltitle = {Annals of Applied Statistics 2008, Vol. 2, No. 3, 916-954},
  keywords     = {stat.AP},
}

@Article{Pang+Lee2004,
  author    = {Bo Pang and Lillian Lee},
  title     = {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},
  year      = {2004},
  booktitle = {Proceedings of the ACL},
}

@Article{Raghu2017,
  author      = {Maithra Raghu and Justin Gilmer and Jason Yosinski and Jascha Sohl-Dickstein},
  title       = {SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},
  abstract    = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
  date        = {2017-06-19},
  eprint      = {http://arxiv.org/abs/1706.05806v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1706.05806v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@article{papernot2018cleverhans,
  title={Technical Report on the CleverHans v2.1.0 Adversarial Examples Library},
  author={Nicolas Papernot and Fartash Faghri and Nicholas Carlini and
  Ian Goodfellow and Reuben Feinman and Alexey Kurakin and Cihang Xie and
  Yash Sharma and Tom Brown and Aurko Roy and Alexander Matyasko and
  Vahid Behzadan and Karen Hambardzumyan and Zhishuai Zhang and
  Yi-Lin Juang and Zhi Li and Ryan Sheatsley and Abhibhav Garg and
  Jonathan Uesato and Willi Gierke and Yinpeng Dong and David Berthelot and
  Paul Hendricks and Jonas Rauber and Rujun Long},
  journal={arXiv preprint arXiv:1610.00768},
  year={2018}
}

@Misc{vgg16,
  author  = {Tensorflow},
  title   = {VGG16 Modell fÃ¼r Imagenet Klassifikationen},
  year    = {2018},
  journal = {github.com},
  url     = {https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/applications/vgg16.py},
}

@Misc{imageNet,
  title = {ImageNet Challenge},
  url   = {http://www.image-net.org/challenges/LSVRC/},
}

@Misc{iML,
  author = {Christoph Molnar},
  title  = {Interpretable Machine Learning, A Guide for Making Black Box Models Explainable},
  year   = {2019},
  url    = {https://christophm.github.io/interpretable-ml-book/},
}

@Misc{ELI5,
  title = {ELI5: A library for debugging/inspecting machine learning classifiers and explaining their predictions},
  note  = {ELI5},
  url   = {https://eli5.readthedocs.io/},
}

@Misc{svccaLink,
  author       = {Maithra Raghu},
  title        = {Interpreting Deep Neural Networks with SVCCA},
  howpublished = {Blog Post},
  year         = {2017},
  url          = {https://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html},
}

@Misc{movieReview,
  author       = {Usman Malik},
  title        = {Text Classification with Python and Scikit-Learn},
  howpublished = {Blog Post},
  year         = {2018},
  url          = {https://stackabuse.com/text-classification-with-python-and-scikit-learn/},
}

@Misc{scikit-learnLink,
  title        = {scikit-learn Machine Learning in Python},
  howpublished = {Internet},
  url          = {https://scikit-learn.org/stable/index.html},
}

@Misc{tcavLink,
  author       = {Been Kim},
  title        = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) [ICML 2018]},
  howpublished = {github.com},
  year         = {2018},
  url          = {https://github.com/tensorflow/tcav},
}

@Misc{TensorFlow,
  title        = {Tensorflow},
  howpublished = {Internet},
  url          = {https://www.tensorflow.org/},
}

@Misc{datenEthik,
  author = {Datenethikkommission der Bundesregierung},
  title  = {Gutachten der Datenethikkommission},
  year   = {2019},
  url    = {https://www.bmjv.de/SharedDocs/Downloads/DE/Themen/Fokusthemen/Gutachten_DEK_DE.pdf?__blob=publicationFile&v=2},
}

@Misc{tensorFlowImageClassification,
  title        = {Image classification},
  howpublished = {Internet},
  year         = {2020},
  url          = {https://www.tensorflow.org/tutorials/images/classification},
}

@Article{Lapuschkin2019,
  author      = {Sebastian Lapuschkin and Stephan WÃ¤ldchen and Alexander Binder and GrÃ©goire Montavon and Wojciech Samek and Klaus-Robert MÃ¼ller},
  title       = {Unmasking Clever Hans Predictors and Assessing What Machines Really Learn},
  abstract    = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly "intelligent" behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
  date        = {2019-02-26},
  doi         = {10.1038/s41467-019-08987-4},
  eprint      = {http://arxiv.org/abs/1902.10178v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1902.10178v1:PDF},
  keywords    = {cs.AI, cs.CV, cs.LG, cs.NE, stat.ML},
}

@article{JMLR:v17:15-618,
    author  = {Sebastian Lapuschkin and Alexander Binder and Gr{{\'e}}goire Montavon and Klaus-Robert M{{{\"u}}}ller and Wojciech Samek},
    title   = {The LRP Toolbox for Artificial Neural Networks},
    journal = {Journal of Machine Learning Research},
    year    = {2016},
    volume  = {17},
    number  = {114},
    pages   = {1-5},
    url     = {http://jmlr.org/papers/v17/15-618.html}
}

@Misc{dogVsCats,
  author       = {Greg Surma},
  title        = {Image Classifier Cats vs Dogs},
  howpublished = {Towards Data Science},
  year         = {2018},
  url          = {https://towardsdatascience.com/image-classifier-cats-vs-dogs-with-convolutional-neural-networks-cnns-and-google-colabs-4e9af21ae7a8},
}

@Misc{tfExplain,
  author   = {Raphael Meudec},
  title    = {tf-explain},
  abstract = {tf_explain implements interpretability methods as Tensorflow 2.0 callbacks to ease neural network's understanding.},
  url      = {https://github.com/sicara/tf-explain},
}

@Misc{limeKeras,
  title        = {Tutorial - Image Classification Keras},
  howpublished = {github.com},
  year         = {2019},
  url          = {https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb},
}

@Misc{textClassEli5,
  author       = {Marc Habegger},
  title        = {Text Klassifizierung mit ELI5},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/habis-git/MT/blob/master/Jupyter%20Notebooks/CombinedVisualizations.ipynb},
}

@Misc{advTesla,
  author = {Steve Povolny, Shivangee Trivedi},
  title  = {Model Hacking ADAS to Pave Safer Roads for Autonomous Vehicles},
  year   = {2020},
  url    = {https://www.mcafee.com/blogs/other-blogs/mcafee-labs/model-hacking-adas-to-pave-safer-roads-for-autonomous-vehicles/},
}

@Article{Goodfellow2014,
  author      = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  title       = {Explaining and Harnessing Adversarial Examples},
  abstract    = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  date        = {2014-12-20},
  eprint      = {http://arxiv.org/abs/1412.6572v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1412.6572v3:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Misc{tensorflowFGSM,
  author       = {Tensorflow Tutorial},
  title        = {Adversarial example using FGSM},
  howpublished = {Internet},
  year         = {2020},
  url          = {https://www.tensorflow.org/tutorials/generative/adversarial_fgsm},
}

@Misc{cleverHans,
  author       = {Ian Goodfellow},
  title        = {CleverHans a Python library to benchmark machine learning systems' vulnerability to adversarial examples},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/tensorflow/cleverhans},
}

@Article{Zhang2019,
  author      = {Xuezhou Zhang and Xiaojin Zhu and Laurent Lessard},
  title       = {Online Data Poisoning Attack},
  abstract    = {We study data poisoning attacks in the online setting where training items arrive sequentially, and the attacker may perturb the current item to manipulate online learning. Importantly, the attacker has no knowledge of future training items nor the data generating distribution. We formulate online data poisoning attack as a stochastic optimal control problem, and solve it with model predictive control and deep reinforcement learning. We also upper bound the suboptimality suffered by the attacker for not knowing the data generating distribution. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.},
  date        = {2019-03-05},
  eprint      = {http://arxiv.org/abs/1903.01666v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1903.01666v2:PDF},
  keywords    = {cs.LG, cs.CR, stat.ML},
}

@Article{Gu2019,
  author    = {Tianyu Gu and Kang Liu and Brendan Dolan-Gavitt and Siddharth Garg},
  title     = {{BadNets}: Evaluating Backdooring Attacks on Deep Neural Networks},
  journal   = {{IEEE} Access},
  year      = {2019},
  volume    = {7},
  pages     = {47230--47244},
  doi       = {10.1109/access.2019.2909068},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{msTay,
  author       = {Peter Lee},
  title        = {Learning from Tayâs introduction},
  howpublished = {Official Microsoft Blog},
  year         = {2016},
  url          = {https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/},
}

@Misc{ocSensMathWorks,
  author       = {Steve Eddins},
  title        = {Network Visualization Based on Occlusion Sensitivity},
  howpublished = {MathWorks Blog},
  year         = {2017},
  url          = {https://blogs.mathworks.com/deep-learning/2017/12/15/network-visualization-based-on-occlusion-sensitivity/},
}

@Article{Zeiler2013,
  author      = {Matthew D Zeiler and Rob Fergus},
  title       = {Visualizing and Understanding Convolutional Networks},
  abstract    = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  date        = {2013-11-12},
  eprint      = {http://arxiv.org/abs/1311.2901v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1311.2901v3:PDF},
  keywords    = {cs.CV},
}

@Article{Bach2015,
  author    = {Sebastian Bach and Alexander Binder and Gr{\'{e}}goire Montavon and Frederick Klauschen and Klaus-Robert MÃ¼ller and Wojciech Samek},
  title     = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  journal   = {{PLOS} {ONE}},
  year      = {2015},
  volume    = {10},
  number    = {7},
  pages     = {e0130140},
  month     = {jul},
  doi       = {10.1371/journal.pone.0130140},
  editor    = {Oscar Deniz Suarez},
  publisher = {Public Library of Science ({PLoS})},
}

@Misc{lrpToolbox,
  author       = {Sebastian Lapuschkin},
  title        = {The LRP Toolbox for Artificial Neural Networks},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/sebastian-lapuschkin/lrp_toolbox},
}

@Misc{xaidemos,
  author = {Fraunhofer Institut},
  title  = {Explainable AI Demos},
  url    = {https://lrpserver.hhi.fraunhofer.de/handwriting-classification},
}

@Misc{deepTaylor,
  title = {A Quick Introduction to Deep Taylor Decomposition},
  year  = {2017},
  url   = {http://www.heatmapping.org/deeptaylor/},
}

@Misc{scopeRules,
  title        = {skope-rules},
  howpublished = {github.com},
  year         = {2019},
  url          = {https://github.com/scikit-learn-contrib/skope-rules},
}

@Article{Szegedy2013,
  author      = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  title       = {Intriguing properties of neural networks},
  abstract    = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  date        = {2013-12-21},
  eprint      = {http://arxiv.org/abs/1312.6199v4},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1312.6199v4:PDF},
  keywords    = {cs.CV, cs.LG, cs.NE},
}

@inproceedings{10.5555/2998828.2998832,
	author = {Craven, Mark W. and Shavlik, Jude W.},
	title = {Extracting Tree-Structured Representations of Trained Networks},
	year = {1995},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {Proceedings of the 8th International Conference on Neural Information Processing Systems},
	pages = {24â30},
	numpages = {7},
	location = {Denver, Colorado},
	series = {NIPSâ95}
}

@inproceedings{Johansson2009,
author = {Johansson, Ulf and Niklasson, Lars},
year = {2009},
month = {05},
pages = {238 - 244},
title = {Evolving Decision Trees Using Oracle Guides},
doi = {10.1109/CIDM.2009.4938655}
}

@Article{Lakkaraju2017,
  author      = {Himabindu Lakkaraju and Ece Kamar and Rich Caruana and Jure Leskovec},
  title       = {Interpretable \& Explorable Approximations of Black Box Models},
  abstract    = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
  date        = {2017-07-04},
  eprint      = {http://arxiv.org/abs/1707.01154v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1707.01154v1:PDF},
  keywords    = {cs.AI},
}

@InCollection{Craven1994,
  author    = {Mark W. Craven and Jude W. Shavlik},
  title     = {Using Sampling and Queries to Extract Rules from Trained Neural Networks},
  booktitle = {Machine Learning Proceedings 1994},
  publisher = {Elsevier},
  year      = {1994},
  pages     = {37--45},
  doi       = {10.1016/b978-1-55860-335-6.50013-1},
}

@Misc{textClassLime,
  author       = {Marc Habegger},
  title        = {Text Klassifizierung mit lime},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/habis-git/MT/blob/master/Jupyter%20Notebooks/TextClassifikation.ipynb},
}

@Misc{imgClassCombined,
  author       = {Marc Habegger},
  title        = {Kombinierte Verfahren fÃ¼r die ErklÃ¤rung von Bild-Klassifikationen mit tf_explain und lime},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/habis-git/MT/blob/master/Jupyter%20Notebooks/Moverieview%20Sentiment%20with%20Keras%20LSTM%20and%20lime%20explainer%20.ipynb},
}

@Misc{imgClassVisualized,
  author       = {Marc Habegger},
  title        = {Mehrere Klassen pro Bild mit verschiedenen explanation Verfahren},
  howpublished = {github.com},
  year         = {2020},
  url          = {https://github.com/habis-git/MT/blob/master/Jupyter%20Notebooks/Visualizations.ipynb},
}

@Misc{lime,
  author       = {Marco Tulio Correia Ribeiro},
  title        = {Lime: Explaining the predictions of any machine learning classifier},
  howpublished = {github.com},
  year         = {2019},
  note         = {Lime},
  url          = {https://github.com/marcotcr/lime},
}

@Misc{nThLIME,
  author = {Nicolas Thiebaut},
  title  = {LIME of words: interpreting Recurrent Neural Networks predictions},
  year   = {2017},
  url    = {https://data4thought.com/deep-lime.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
