
\newglossaryentry{MLg}
{
	name=Machine Learning,
	description={deutsch Maschinelles lernen. Ein künstliches System lernt aus Beispielen und kann diese nach Beendigung der Lernphase verallgemeinern. }
}

\newglossaryentry{grad}
{
	name=Gradienten,
	description={Vektor dessen Komponenten die partiellen Ableitungen im Punkt P sind, der Gradient zeigt deshalb in die Richtung der größten Änderung. }
}

\newglossaryentry{class}
{
	name=Klassifikation,
	description={Vorhersage einer bestimmten Klasse (Kategorie) wie zum Beispiel Hund, Katze oder Vogel. Es wird zwischen einer Binäre Klassifikation (kennt nur zwei Zustände, meisten Ja/Nein) und Mehrklassen Klassifikationsverfahren (mehr als zwei Werte) unterschieden.  }
}

\newglossaryentry{nb}
{
	name=Naive Bayes,
	description={Auch Naiver Bayes-Klassifikator genannt. Basiert auf dem Bayesschen Theorem mit der naiven Grundannahme, dass jedes Attribut nur vom Klassenattribut abhängt. Obwohl dies häufig nicht der Fall ist funktionieren Naive Bayes Klassifikatoren sehr gut. }
}

\newglossaryentry{AIg}
{
	name=Artificial Intelligence,
	description={deutsch Künstliche Intelligenz. Nicht scharf abzugrenzender Bereich des Machine Learning in dem versucht wird ein intelligentes Verhalten
	 analog menschlicher oder tierischer Verhaltensweisen nachzubilden. }
}

\newglossaryentry{XAI}
{
	name=Explainable Artificial Intelligence,
	description={deutsch erklärbare künstliche Inteligenz, Methodiken um Menschen die Vorhersagen durch Modelle des maschinellen Lernens zu erläutern. }
}

\newglossaryentry{DNN}
{
    name=Deep Neural Network,
    description={deutsch tiefes lernen, Bezeichnet Neuronale Netze mit vielen Zwischenschichten.}
}

\newglossaryentry{NN}                                 
{
	name=neuronales Netz,
	description={Algorithmus welcher nach dem Vorbild des menschlichen Gehirns entworfen wurde um Muster und Beziehungen in Daten zu erkennen.}   
}   

\newglossaryentry{acc}                                 
{
	name=Accuracy,
	description={Prozentsatz der korrekten Vorhersagen eines Modelles.}   
}   

\newglossaryentry{LRP}                                 
{
	name=Layer-wise Relevance Propagation,
	description={Technik zur Bestimmung der Merkmale welche am stärksten für das Endresultat verantwortlich sind.}                                   
}                          

\newglossaryentry{Blackbox}                                 
{
	name=Blackbox,
	description={Als Blackbox werden Modelle bezeichnet welche durch ihre Struktur und die Art und Weise wie die Daten verarbeitet werden nur schwer verständlich sind. Typische Blackbox Modelle sind tiefe neurale Netze (DNN) und andere Varianten von neuronalen Netzen. Blackbox Modelle gelten in der Regel als leistungsfähiger als Whitebox Modelle.}                                   
}     

\newglossaryentry{Whitebox}                                 
{
	name=Whitebox,
	description={Ein Whitebox Modell ist durch seinen Aufbau verständlich und die Entscheidungsfindung ist nachvollziehbar (nicht zu verwechseln mit einfach). Whitebox Modelle sind häufig im eher Bereich der Statistik angesiedelt. Es gibt kein festes Entscheidungskriterium ob eine Technik eher zu den Blackbox oder Whitebox Modellen gehört, bei vielen Verfahren besteht aber ein Konsens darüber zu welcher Gruppe sie gehören.}                                   
}     

\newglossaryentry{DT}                                 
{
	name=Decision Tree,
	description={Entscheidungsbaum, Familie von ML Algorithmen}                                   
}     

\newglossaryentry{GC}
{
	name=Grad CAM,
	description={Gradient-weighted Class Activation Mapping, Technik welche für eine Entscheidung relevanten Bildinhalte optisch hervorhebt}  
}

\newglossaryentry{KH}
{
	name=Kluger-Hans-Effekt,
	description={``Kluger Hans'' war ein Pferd aus dem Anfang des 20. Jahrhunderts das angeblich rechnen und zählen konnte, jedoch auf  die feinen Nuancen der Mimik und Körpersprache des Fragestellers reagierte. Seitdem wird als ``Kluger-Hans-Effekt'' eine unbewusste beeinflussung des Studienobjektes bezeichnet. In Machine Learning Lösungen kann der ``Kluger-Hans-Effekt'' auftauchen wenn ein Model mit Daten trainiert wird welche die Vorhersage unbewusst in eine bestimmte Richtung lenken.}  
}

\newglossaryentry{OS}
{
	name=Occlusion Sensitivity,
	description={Verfahren um die für eine Klassifikation relevanten Bildinhalte zu finden indem bestimmte Bildinhalte entfernt werden (Occlusion) und die dabei entstehende Veränderung auf die Klassifikation gemessen wird.}  
}

\newglossaryentry{GI}
{
	name=Gradients Input,
	description={Verfahren nach Simonyan \parencite{Simonyan2013}. Gradients Input misst den relativen Einfluss einer Eigenschaft durch berechnen des Gradienten der Entscheidung durch Einbezug der Eingangs-Eigenschaften.}  
}

\newglossaryentry{BIAS}
{
	name=Bias,
	description={Bias, deutsch Tendenz oder Voreingenommenheit, kann in Machine Learning Modellen auftreten wenn die Trainingsdaten unausgewogen sind. Dies kann zu einer sogenannten selbsterfüllenden Prophezeiung werden indem die Resultate des Models zu neuen Trainingsdatensätzen führen welche die Tendenz noch verstärken.}  
}

\newglossaryentry{limeG}
{
	name=LIME,
	description={Local interpretable model-agnostic explanations, eine unabhängig des verwendeten Algorithmus anwendbare Erklärungstechnik für Black Box Modelle }
}

\newglossaryentry{tcavG}
{
	name=Testing with Concept Activation Vectors,
	description={Technik welche Erklärungen einer Klassifikation durch Erkennung der Bildbestandteile erzeugt. Benutzt dafür Neuronale Netze welche auf einzelnen Bestandteile trainiert werden. \parencite{Kim2017}}
}

\newglossaryentry{av}
{
	name=Activation Vector (Aktivierungsvektor),
	description={In einem Neuronalen Netzwerk erzeugt ein Neuron für jedes Bild welches Analysiert wird einen bestimmten Ausgangswert. Für mehrere Bilder bilden die jeweiligen Ausgangswerte des Neurons den sogenannten Aktivierungsvektor.}
}

\newglossaryentry{svccaG}
{
	name=SVCCA,
     description={Singular Vector Canonical Correlation Analysis, ein Verfahren welches Aktivierungs Vektoren eines Neuronalen Netzes vergleicht. Der Vergleich kann entweder zwischen den Layern eines Netzes oder zwischen unterschiedlichen Netzen durchgeführt werden. \parencite{Raghu2017}}
}

\newglossaryentry{binClassificator}
{
	name=Binärer Klassifikator,
     description={Ein binärer Klassifikator ist eine Sonderform eines Klassifikators welche nur eine Klasse kennt. Häufig sind das ja/nein Fragen zum Beispiel ``ist auf diesem Bild ein Tumor erkennbar?''.}
}

\newglossaryentry{lstmG}
{
	name=Long short-term memory,
     description={Variante eines neuronalen Netzes welches Informationen aus früheren Verarbeitungsphasen erneut einbinden kann.}
}

\newglossaryentry{dsgvo}
{
	name=Datenschutz-Grundverordnung,
     description={Verordnung der Europäischen Union mit der die Regeln zur Verarbeitung personenbezogener Daten durch die meisten Datenverarbeiter, sowohl private wie öffentliche, EU-weit vereinheitlicht werden. Definiert Anforderungen an den Datenschutz und die Datenverarbeitung welche für \Gls{ML} Lösungen betreffen.}
}

\newacronym{lstm}{LSTM}{Long short-term memory}

\newacronym{DSGVO}{DSGVO}{Datenschutz-Grundverordnung}

\newacronym{ML}{ML}{Machine Learning}

\newacronym{AI}{AI}{Artificial Intelligence}

\newacronym{cnn}{CNN}{Convolutional Neural Network}

\newacronym{dnn}{DNN}{Deep Neural Network}

\newacronym{lrp}{LRP}{Layer-wise Relevance Propagation}

\newacronym{lime}{LIME}{Local interpretable model-agnostic explanations}

\newacronym{tcav}{TCAV}{Testing with Concept Activation Vectors}

\newacronym{svcca}{SVCCA}{Singular Vector Canonical Correlation Analysis}

\newacronym{xai}{XAI}{Explainable Artificial Intelligence}

\newacronym{glm}{GLM}{Generalized Linear Models}

\newacronym{gam}{GAM}{Generalized Additive Models}

\newacronym{lfr}{LFR}{Learned fair representations}

\newacronym{m-gbm}{M-GBM}{Monotonic gradient boosting}

\newacronym{pate}{PATE}{Private aggregation of teacher ensembles}

\newacronym{sbrl}{SBRL}{Scalable Bayesian rule list}

\newacronym{slim}{SLIM}{Supersparse linear integer models}

\newacronym{dek}{DEK}{Datenethikkommission}

\newacronym{gradcam}{Grad CAM}{Gradient-weighted Class Activation Mapping}

\newacronym{aip}{AIP}{Adversarial Image Perturbations}
